# evaluate.py

import os
import re

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from bert_score import score as bert_score
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from rouge_score import rouge_scorer
from sklearn.metrics import classification_report, confusion_matrix, f1_score

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

embedding = OpenAIEmbeddings(openai_api_key=api_key)
llm = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=api_key)

db = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)

test_df = pd.read_csv("./Customer_IT_Support/test.csv")

# ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿
results = []

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

use_rag = True


def build_prompt(similar_docs, query_row):
    context = "\n\n".join(
        f"subject: {doc.page_content}\ntype: {doc.metadata['type']}\nqueue: {doc.metadata['queue']}\npriority: {doc.metadata['priority']}\nanswer: {doc.metadata['answer']}"
        for doc in similar_docs
    )

    if use_rag:
        context_block = f"""
    --- éå»ã®äº‹ä¾‹ ---
    {context}
    """
    else:
        context_block = """
    --- éå»ã®äº‹ä¾‹ ---
    ï¼ˆä»Šå›ã®ã‚±ãƒ¼ã‚¹ã§ã¯éå»äº‹ä¾‹ã¯æä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ï¼‰
    """

    prompt = f"""
    éå»ã®äº‹ä¾‹ã‚’è€ƒæ…®ã—ã¦ã€ä»¥ä¸‹ã®æ¡ä»¶ã«å¾“ã„ã€æ–°ã—ã„å•ã„åˆã‚ã›ã«å¯¾ã—ã¦æœ€é©ãª type, queue, priority, answer ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

    - type ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ï¼š
    Change / Incident / Problem / Request

    - queue ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ï¼š
    Billing and Payments /
    Customer Service /
    General Inquiry /
    Human Resources /
    IT Support /
    Product Support /
    Returns and Exchanges /
    Sales and Pre-Sales /
    Service Outages and Maintenance /
    Technical Support

    - priority ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ï¼š
    high / medium / low

    - answer ã¯ã€body ã«æ›¸ã‹ã‚ŒãŸå†…å®¹ã«å¯¾ã™ã‚‹å…·ä½“çš„ã§é©åˆ‡ãªå›ç­”æ¡ˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

    {context_block}

    --- æ–°ã—ã„å•ã„åˆã‚ã› ---
    subject: {query_row['subject']}
    body: {query_row['body']}
    language: {query_row['language']}
    version: {query_row['version']}

    å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š
    type: â€¦
    queue: â€¦
    priority: â€¦
    answer: â€¦
    """
    return prompt


def parse_output(text):
    # æ­£è¦è¡¨ç¾ã§æŠ½å‡º
    type_ = queue = priority = answer = ""
    for line in text.splitlines():
        if line.lower().startswith("type:"):
            type_ = line.split(":", 1)[1].strip()
        elif line.lower().startswith("queue:"):
            queue = line.split(":", 1)[1].strip()
        elif line.lower().startswith("priority:"):
            priority = line.split(":", 1)[1].strip()
        elif line.lower().startswith("answer:"):
            answer = line.split(":", 1)[1].strip()
    return type_, queue, priority, answer

for idx, row in test_df.iterrows():
    print(f"Processing row {idx+1}/{len(test_df)}...")
    query_text = f"subject: {row['subject']}\nbody: {row['body']}\nlanguage: {row['language']}\nversion: {row['version']}"
    similar_docs = db.similarity_search(query_text, k=3)
    prompt = build_prompt(similar_docs, row)
    pred = llm.invoke(prompt)
    type_, queue, priority, answer = parse_output(pred.content)

    rougeL = scorer.score(row['answer'], answer)['rougeL'].fmeasure
    results.append({
        'true_type': row['type'], 'pred_type': type_,
        'true_queue': row['queue'], 'pred_queue': queue,
        'true_priority': row['priority'], 'pred_priority': priority,
        'true_answer': row['answer'], 'pred_answer': answer,
        'rougeL': rougeL
    })

df_results = pd.DataFrame(results)

# BERTScore è¨ˆç®—
P, R, F1 = bert_score(df_results['pred_answer'].tolist(), df_results['true_answer'].tolist(), lang='en')
df_results['BERTScore'] = F1.numpy()

df_results.to_csv("evaluation_results.csv", index=False)
print("ğŸ“„ è©•ä¾¡çµæœã‚’ evaluation_results.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")

# åˆ†é¡ã‚¿ã‚¹ã‚¯è©•ä¾¡
for col in ['type', 'queue', 'priority']:
    y_true = df_results[f'true_{col}']
    y_pred = df_results[f'pred_{col}']

    labels = sorted(list(set(y_true) | set(y_pred)))
    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')
    present_labels = sorted(set(y_true))
    macro_f1 = f1_score(y_true, y_pred, labels=present_labels, average='macro', zero_division=0)
    print(f"\n{col.upper()} Macro F1: {macro_f1:.4f}")
    print(classification_report(y_true, y_pred))

    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title(f"{col.upper()} Confusion Matrix (Macro F1={macro_f1:.4f})")
    plt.ylabel("True")
    plt.xlabel("Predicted")
    plt.tight_layout()
    plt.savefig(f"confusion_matrix_{col}.png")
    plt.close()
    print(f"ğŸ–¼ Confusion matrix saved: confusion_matrix_{col}.png")

print("âœ… å…¨ã¦ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ")
